import os
from dotenv import load_dotenv
import json
from typing import List
import requests
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain.document_loaders import TextLoader
from langchain.prompts import PromptTemplate
from openai import OpenAI

load_dotenv()

# Upstage API ì„¤ì •
UPSTAGE_API_KEY = os.environ["UPSTAGE_API_KEY"]
SOLAR_EMBEDDING_URL = "https://api.upstage.ai/v1/embeddings"
SOLAR_LLM_MODEL = "solar-pro"

client = OpenAI(
    api_key=UPSTAGE_API_KEY,
    base_url="https://api.upstage.ai/v1"
)

# Solar Embedding ë˜í¼
def get_solar_embedding(text: str) -> List[float]:
    headers = {
        "Authorization": f"Bearer {UPSTAGE_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "input": text,
        "model": "embedding-query"
    }
    response = requests.post(SOLAR_EMBEDDING_URL, headers=headers, json=payload)
    response.raise_for_status()
    return response.json()["data"][0]["embedding"]

class SolarEmbeddingWrapper:
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return [get_solar_embedding(t) for t in texts]
    
    def embed_query(self, text: str) -> List[float]:
        return get_solar_embedding(text)

# ë²¡í„° DB ì´ˆê¸°í™”
def init_vectorstore(persist_dir="rag"):
    if not os.path.exists(persist_dir) or not os.listdir(persist_dir):
        print("ğŸ”„ ë²¡í„° DB ì´ˆê¸°í™” ì¤‘...")
        loader = TextLoader("reference_docs/national_institute.txt", encoding="utf-8")
        documents = loader.load()

        splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        docs = splitter.split_documents(documents)

        embedding = SolarEmbeddingWrapper()
        Chroma.from_documents(docs, embedding, persist_directory=persist_dir)
        print("âœ… ë²¡í„° DB ìƒì„± ì™„ë£Œ")

# ì „ì—­ ì´ˆê¸°í™”
embedding = SolarEmbeddingWrapper()
vectorstore = Chroma(persist_directory="rag", embedding_function=embedding)
retriever = vectorstore.as_retriever()

# LLM í˜¸ì¶œ í•¨ìˆ˜
def call_solar_llm(prompt: str) -> str:
    response = client.chat.completions.create(
        model=SOLAR_LLM_MODEL,
        messages=[
            {"role": "user", "content": prompt}
        ]
    )
    return response.choices[0].message.content.strip()

# context ê²€ìƒ‰
def retrieve_context(query: str, k: int = 3) -> str:
    docs = retriever.invoke(query)[:k]
    return "\n".join([doc.page_content for doc in docs])

def retrieve_error_patterns(k: int = None) -> str:
    path = "optimizer/prompt_error_patterns.jsonl"
    if not os.path.exists(path):
        return ""
    
    with open(path, encoding="utf-8") as f:
        lines = [json.loads(l.strip()) for l in f if l.strip()]
    
    if not lines:
        return ""
    
    # ìë™ ê²°ì •
    if k is None:
        total = len(lines)
        k = min(10, max(3, total // 10))  # ì˜ˆ: ì „ì²´ì˜ 10% ë˜ëŠ” ìµœëŒ€ 10ê°œ

    samples = lines[-k:]
    formatted = []
    for err in samples:
        from_text = err.get("wrong_change_from", "").strip()
        to_text = err.get("wrong_change_to", "").strip()
        if from_text and to_text:
            formatted.append(f"- '{from_text}' â†’ '{to_text}' ëŠ” ê³¼ì‰ êµì • ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë¯€ë¡œ í”¼í•´ì•¼ í•¨")
    return "\n".join(formatted)


def generate_prompts_batch(prompt_text: str, n: int = 3) -> List[str]:
    context = retrieve_context(prompt_text)
    error_hints = retrieve_error_patterns(k=10)

    batch_prompt = f"""
ë‹¹ì‹ ì€ í•œêµ­ì–´ ë¬¸ë²• êµì • ì „ë¬¸ê°€ì´ì í”„ë¡¬í”„íŠ¸ ìµœì í™” ëŒ€íšŒ ì°¸ê°€ìì…ë‹ˆë‹¤.

ì•„ë˜ì˜ ë¬¸ë§¥ê³¼ ì‹¤íŒ¨ ì‚¬ë¡€ë¥¼ ì°¸ê³ í•˜ì—¬ AI ëª¨ë¸ì´ ì‚¬ìš©í•  í”„ë¡¬í”„íŠ¸ {n}ê°œë¥¼ ìƒì„±í•˜ì„¸ìš”.

[ëª©í‘œ]
- AIê°€ ì…ë ¥ ë¬¸ì¥ì˜ ë¬¸ë²•, ë§ì¶¤ë²•, ë„ì–´ì“°ê¸°, ì¡°ì‚¬, ì–´ë¯¸, ë¬¸ì¥ ë¶€í˜¸ ì˜¤ë¥˜ë¥¼ ì •í™•í•˜ê²Œ ìˆ˜ì •í•  ìˆ˜ ìˆë„ë¡ í”„ë¡¬í”„íŠ¸ë¥¼ ì„¤ê³„í•˜ì„¸ìš”.
- êµì • ê²°ê³¼ëŠ” ì •ë‹µ ë¬¸ì¥ê³¼ ìµœëŒ€í•œ ìœ ì‚¬í•´ì•¼ í•˜ë©°, **ê³¼ì‰ ìˆ˜ì •ì€ ë°˜ë“œì‹œ í”¼í•´ì•¼ í•©ë‹ˆë‹¤.**
- êµì • í”„ë¡¬í”„íŠ¸ëŠ” ì‹¤ì œ SNS, ì»¤ë®¤ë‹ˆí‹° ë“± êµ¬ì–´ì²´ ë¬¸ì¥ì—ë„ ì˜ ì‘ë™í•´ì•¼ í•˜ë©°, ì‹¤ìš©ì ì´ê³  êµ¬ì¡°ì ì¸ ì§€ì‹œë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

[ì‘ì„± ì§€ì¹¨]
- ê° í”„ë¡¬í”„íŠ¸ëŠ” ë°˜ë“œì‹œ {{text}} ìë¦¬í‘œì‹œìë¥¼ **ì •í™•íˆ 1íšŒë§Œ** í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
- í”„ë¡¬í”„íŠ¸ëŠ” 1~5ë¬¸ì¥ ì‚¬ì´ë¡œ ììœ ë¡­ê²Œ ì‘ì„±í•˜ë˜, ë¬¸ì¥ êµ¬ì¡°ì™€ ì˜ë„ê°€ ëª…í™•í•´ì•¼ í•©ë‹ˆë‹¤.
- ì¶œë ¥ì—ëŠ” êµì •ëœ ë¬¸ì¥ ì™¸ì— ì„¤ëª…, ë¶„ì„, ë§ˆí¬ë‹¤ìš´ì´ í¬í•¨ë˜ì§€ ì•Šë„ë¡ ìœ ë„í•˜ì„¸ìš”.

[ì‹¤íŒ¨ êµì • ì‚¬ë¡€]
ì•„ë˜ëŠ” ê³¼ê±° í”„ë¡¬í”„íŠ¸ê°€ ì˜ëª»ëœ êµì •ì„ ìœ ë„í•œ ì˜ˆì‹œì…ë‹ˆë‹¤. 
ì´ëŸ° ì˜¤ë¥˜ê°€ ì¬ë°œí•˜ì§€ ì•Šë„ë¡ ì„¤ê³„ì— ë°˜ì˜í•˜ì„¸ìš”.

{error_hints}

[ê¸°ì¡´ í…œí”Œë¦¿]
{prompt_text}

[ì°¸ê³  ë¬¸ì„œ ìš”ì•½]
{context}

[ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ëª©ë¡]
"""
    response = call_solar_llm(batch_prompt)
    return [
        line.strip()
        for line in response.strip().split("\n")
        if "{text}" in line and line.strip().count("{text}") == 1 and len(line.strip()) > 10
    ][:n]

# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    init_vectorstore()
    seed_prompt = "ë‹¤ìŒ ë¬¸ì¥ì„ ê³ ì³ì¤˜: {text}"

    print("\n ë°°ì¹˜ ê°œì„  ê²°ê³¼:")
    print(generate_prompts_batch(seed_prompt, n=3))