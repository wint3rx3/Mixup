import os
from dotenv import load_dotenv
import json
from typing import List
import requests
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain.document_loaders import TextLoader
from langchain.prompts import PromptTemplate
from openai import OpenAI

load_dotenv()  # ë°˜ë“œì‹œ í˜¸ì¶œí•´ì•¼ í•¨

# ========= ğŸŒ Upstage API ì„¤ì • ========= #

UPSTAGE_API_KEY = os.environ["UPSTAGE_API_KEY"]
SOLAR_EMBEDDING_URL = "https://api.upstage.ai/v1/embeddings"
SOLAR_LLM_MODEL = "solar-pro"

client = OpenAI(
    api_key=UPSTAGE_API_KEY,
    base_url="https://api.upstage.ai/v1"
)

# ========= ğŸŒ Solar Embedding ë˜í¼ ========= #

def get_solar_embedding(text: str) -> List[float]:
    headers = {
        "Authorization": f"Bearer {UPSTAGE_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "input": text,
        "model": "embedding-query"
    }
    response = requests.post(SOLAR_EMBEDDING_URL, headers=headers, json=payload)
    response.raise_for_status()
    return response.json()["data"][0]["embedding"]

class SolarEmbeddingWrapper:
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return [get_solar_embedding(t) for t in texts]
    
    def embed_query(self, text: str) -> List[float]:
        return get_solar_embedding(text)

# ========= ğŸ”§ ë²¡í„° DB ì´ˆê¸°í™” ========= #

def init_vectorstore(persist_dir="rag_db"):
    if not os.path.exists(persist_dir) or not os.listdir(persist_dir):
        print("ğŸ”„ ë²¡í„° DB ì´ˆê¸°í™” ì¤‘...")
        loader = TextLoader("reference_docs/national_institute.txt", encoding="utf-8")
        documents = loader.load()

        splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        docs = splitter.split_documents(documents)

        embedding = SolarEmbeddingWrapper()
        Chroma.from_documents(docs, embedding, persist_directory=persist_dir)
        print("âœ… ë²¡í„° DB ìƒì„± ì™„ë£Œ")

# âœ… ì „ì—­ ì´ˆê¸°í™”
embedding = SolarEmbeddingWrapper()
vectorstore = Chroma(persist_directory="rag_db", embedding_function=embedding)
retriever = vectorstore.as_retriever()

# ========= ğŸ§  LLM í˜¸ì¶œ í•¨ìˆ˜ ========= #

def call_solar_llm(prompt: str) -> str:
    response = client.chat.completions.create(
        model=SOLAR_LLM_MODEL,
        messages=[
            {"role": "user", "content": prompt}
        ]
    )
    return response.choices[0].message.content.strip()

# ========= ğŸ” context ê²€ìƒ‰ ========= #

def retrieve_context(query: str, k: int = 3) -> str:
    docs = retriever.get_relevant_documents(query)[:k]
    return "\n".join([doc.page_content for doc in docs])

# ========= âœ… ë‹¨ì¼ í…œí”Œë¦¿ ê°œì„  ========= #

def generate_prompt(prompt_text: str) -> str:
    context = retrieve_context(prompt_text)
    prompt_template = PromptTemplate(
        input_variables=["context", "text"],
        template="""ë‹¤ìŒì€ í•œêµ­ì–´ ë§ì¶¤ë²• êµì • í”„ë¡¬í”„íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ ì°¸ê³  ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ,
ë” ì„±ëŠ¥ì´ ì¢‹ì€ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

[ì°¸ê³  ë¬¸ì„œ ìš”ì•½]
{context}

[ê¸°ì¡´ í…œí”Œë¦¿]
{text}

[ê°œì„ ëœ í…œí”Œë¦¿]
"""
    )
    full_prompt = prompt_template.format(context=context, text=prompt_text)
    return call_solar_llm(full_prompt)

# ========= âœ… ì—¬ëŸ¬ ê°œ í…œí”Œë¦¿ ìƒì„± (batch) ========= #

def generate_prompts_batch(prompt_text: str, n: int = 3) -> List[str]:
    context = retrieve_context(prompt_text)
    batch_prompt = f"""
ë‹¤ìŒì€ í•œêµ­ì–´ ë§ì¶¤ë²• êµì • í”„ë¡¬í”„íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ ì°¸ê³  ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ,
ìƒˆë¡œìš´ êµì • í”„ë¡¬í”„íŠ¸ {n}ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”. ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.

[ì°¸ê³  ë¬¸ì„œ ìš”ì•½]
{context}

[ê¸°ì¡´ í…œí”Œë¦¿]
{prompt_text}

[ê°œì„ ëœ í…œí”Œë¦¿ ëª©ë¡]
"""
    response = call_solar_llm(batch_prompt)
    return [line.strip() for line in response.strip().split("\n") if len(line.strip()) > 10][:n]

# ========= âœ… í…ŒìŠ¤íŠ¸ ========= #

if __name__ == "__main__":
    init_vectorstore()

    seed_prompt = "ë‹¤ìŒ ë¬¸ì¥ì„ ê³ ì³ì¤˜: {text}"

    print("\nâœ… ë‹¨ì¼ ê°œì„  ê²°ê³¼:")
    print(generate_prompt(seed_prompt))

    print("\nâœ… ë°°ì¹˜ ê°œì„  ê²°ê³¼:")
    print(generate_prompts_batch(seed_prompt, n=3))
